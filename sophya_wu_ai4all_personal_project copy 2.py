# -*- coding: utf-8 -*-
"""Sophya Wu - AI4ALL Personal Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ir7nqLwdPSnmGYQAfh9ye_zC57rruj87

# **WORLD HAPPINESS DATA 2019**

A) My first objective/goal is a classification problem similar to the regression problem but more general: to classify whether the country has a high, medium, or low happiness score.
Given the same 3 categories of data, my algorithm will classify the country into the 3 bins. I intend to use the decision tree method along with the 3 classification questions:

* Is GDP per capita higher than XX value?
* Is healthy life expectancy higher than XX value?
* Is social support higher than XX value?


B) My second objective/goal is the regression problem: to predict a country’s happiness score.
Specifically, given data about a country’s GDP per capita, social support, and healthy life expectancy. My algorithm will predict the country’s happiness score. I intend to use linear regression in 4 dimensions to predict this value.


C) My third objective is another regression problem: to determine the relative importance of the features in constructing the happiness score.
This may require all the features of the data and my algorithm will use the linear regression in 7 dimensions and these corresponding values for the weights (w1, w2, w3, w4, w5, w6) will be the relative importance of each feature on the happiness score.

# ***PART 1***

This part is the daily Google authentication to access Google Sheets data.
"""

from google.colab import auth
auth.authenticate_user()
import gspread
from oauth2client.client import GoogleCredentials
gc = gspread.authorize(GoogleCredentials.get_application_default())

# Commented out IPython magic to ensure Python compatibility.
# Important import statements for everything

# %matplotlib inline
import sklearn
import numpy as np

# Import the python packages we want to use
# Pandas helps us explore and manipulate the data
# Matplotlib helps us to graph and plot the data
import pandas as pd
import matplotlib.pyplot as plt

"""Get the worksheet data."""

# World Happiness Report 2019 data
wb = gc.open_by_url('https://docs.google.com/spreadsheets/d/1bwIXIzmE77O1Gsmp3_H3vveAExYyZ17mkGagq-4PNtY/edit#gid=906238556')

# Import your data into a Pandas data frame
# You may have to change the name of the sheet to match the sheet you are pulling from your spreadsheet - pay attention to capitalization!
sheet = wb.worksheet('data') #change this sheet name for a new dataset
data = sheet.get_all_values()

# Pandas stores data in a 'data frame'
df = pd.DataFrame(data)

# Set the headings
df.columns = df.iloc[0]
df = df.iloc[1:]

# Print the first few lines of the data frame to check that everything looks good
df.head()

# Data information

df.describe()

# Check the data types in your data frame. Are they all correct? 
# Usually pandas imports everything as an object, which means you will need to tell is which data type each column should be.
# You should do this so you can do math with data that includes numbers, etc.

df.dtypes

"""Create a dictionary to store all the values of the data spreadsheet."""

# World Happiness 2019 data
new_df = {
    'Rank': df['Overall rank'].astype(int),
    'Country': df['Country or region'].astype(str),
    'Score': df['Score'].astype(float),
    'GDP': df['GDP per capita'].astype(float),
    'SocialSupport': df['Social support'].astype(float),
    'Health': df['Healthy life expectancy'].astype(float),
    'Freedom': df['Freedom to make life choices'].astype(float),
    'Generosity': df['Generosity'].astype(float),
    'Corruption': df['Perceptions of corruption'].astype(float)
}

print(new_df)

"""Create a new Data Frame."""

happiness_df = pd.DataFrame(new_df)
happiness_df.head()

"""Check the data types of the data."""

# And check to make sure all our data types are correct
happiness_df.dtypes

"""Use .describe() to understand each column of the data sheet."""

happiness_df["Rank"].describe()

happiness_df["Country"].describe()

happiness_df["Score"].describe()

happiness_df["GDP"].describe()

happiness_df["SocialSupport"].describe()

happiness_df["Health"].describe()

happiness_df["Freedom"].describe()

happiness_df["Generosity"].describe()

happiness_df["Corruption"].describe()

"""Use matplotlib to make some graphs."""

#HAPPINESS SCORES

plt.figure(figsize=(10,8))
plt.hist(happiness_df['Score'], bins=20, range=(0.01, 5))
plt.title('\nHistogram of Happiness Scores', fontsize=24)
plt.ylabel('Frequency', fontsize=14)
plt.xlabel('Happiness Score', fontsize=14)
plt.show()

#GDP

plt.figure(figsize=(10,8))
plt.hist(happiness_df['GDP'], bins=20, range=(0.01, 5))
plt.title('\nHistogram of GDP per capita', fontsize=24)
plt.ylabel('Frequency', fontsize=14)
plt.xlabel('GDP per capita', fontsize=14)
plt.show()

#SOCIAL SUPPORT  
           
plt.figure(figsize=(10,8))
plt.hist(happiness_df['SocialSupport'], bins=20, range=(0.01, 5))
plt.title('\nHistogram of Social Support', fontsize=24)
plt.ylabel('Frequency', fontsize=14)
plt.xlabel('Social Support', fontsize=14)
plt.show()

#HEALTH

plt.figure(figsize=(10,8))
plt.hist(happiness_df['Health'], bins=20, range=(0.01, 5))
plt.title('\nHistogram of Health', fontsize=24)
plt.ylabel('Frequency', fontsize=14)
plt.xlabel('Health', fontsize=14)
plt.show()

#FREEDOM

plt.figure(figsize=(10,8))
plt.hist(happiness_df['Freedom'], bins=20, range=(0.01, 5))
plt.title('\nHistogram of Freedom', fontsize=24)
plt.ylabel('Frequency', fontsize=14)
plt.xlabel('Freedom', fontsize=14)
plt.show()

#GENEROSITY

plt.figure(figsize=(10,8))
plt.hist(happiness_df['Generosity'], bins=20, range=(0.01, 5))
plt.title('\nHistogram of Generosity', fontsize=24)
plt.ylabel('Frequency', fontsize=14)
plt.xlabel('Generosity', fontsize=14)
plt.show()

#CORRUPTION

plt.figure(figsize=(10,8))
plt.hist(happiness_df['Corruption'], bins=20, range=(0.01, 5))
plt.title('\nHistogram of Corruption', fontsize=24)
plt.ylabel('Frequency', fontsize=14)
plt.xlabel('Corruption', fontsize=14)
plt.show()

"""Using groupby"""

happiness_df.groupby("GDP")["Score"].mean()

happiness_df.groupby("SocialSupport")["Score"].mean()

happiness_df.groupby("Health")["Score"].mean()

happiness_df.groupby("Freedom")["Score"].mean()

happiness_df.groupby("Generosity")["Score"].mean()

happiness_df.groupby("Corruption")["Score"].mean()

"""New instance of the features so that I don't have to keep scrolling up"""

happiness_df.dtypes

"""# ***PART 2***

*Feature Engineering*

To work towards my classification objective, I will bin the scores into three categories of low, medium, and high happiness scores.
"""

# use qcut to evenly split the data
# binning with 3 resultant bins

# 1 = low, 2 = medium, 3 = high
classes_level = [1, 2, 3]

happiness_df["Score Level"], limits_of_bins = pd.qcut(happiness_df["Score"], q = 3, labels = classes_level, retbins=True)

pd.value_counts(happiness_df['Score Level'])

happiness_df.head()

bin_limits_table = pd.DataFrame(zip(limits_of_bins, classes_level), columns=["Limit", "Label"])

print(bin_limits_table)

"""# ***PART 3***

## ***CLUSTERING OBJECTIVE***

This uses the k-means clustering algorithm.
"""

# Importing needed modules

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

"""Choose which features to base clustering off of.

*   GDP per capita
*   social support
*   healthy life expectancy
"""

data_grouped = happiness_df[['GDP', 'Score']]

num_groups = [3, 4, 5, 6, 7, 8, 9, 10]

# Testing each group size in order to determine the optimal number of groups
# Quantified through silhouette score (the higher the better)
for size_grp in num_groups:
  the_cluster_func = KMeans(n_clusters=size_grp, random_state=10)
  group_labels = the_cluster_func.fit_predict(data_grouped)
  silhouette_res = silhouette_score(data_grouped, group_labels)
  print("Num of Groups: %d has silhouette score of %f" % (size_grp, silhouette_res))

labels = group_labels.tolist()
labels = pd.Series(labels,name='labels')
values = pd.value_counts(labels)

print(values)

# Scatter plot clusters
data_grouped = pd.concat([data_grouped, labels], axis=1)

groups = data_grouped.groupby(labels)
for name, group in groups:
    plt.plot(group["Score"], group["GDP"], marker="o", linestyle="", label=name)
plt.legend()
plt.xlabel("Score")
plt.ylabel("GDP")

# Statistics
print("Mean\n",groups.aggregate(np.mean))

print("\n Minimum Value \n",groups.aggregate(np.min))

print("\n Maximum Value \n",groups.aggregate(np.max))

"""# ***PART 4***

## ***CLASSIFICATION OBJECTIVE: CLASSIFY HAPPINESS SCORE***

My objective/goal is a classification problem similar to the regression problem but more general: to classify whether the country has a high, medium, or low happiness score.
Given the same 3 categories of data, my algorithm will classify the country into the 3 bins. I intend to use the decision tree method along with the 3 classification questions:
* Is GDP per capita higher than XX value?
* Is healthy life expectancy higher than XX value?
* Is social support higher than XX value?

This happiness dataset as recorded above has:


*   156 examples
*   3 classes: high happiness, medium happiness, and low happiness
*   6 features: GDP, SocialSupport, Health, Freedom, Generosity, Corruption
*   52 examples in each class (see part 2)
"""

# Import the test and training split
from sklearn.model_selection import train_test_split

# Get a new data frame
class_happiness_df = pd.DataFrame(happiness_df, columns=["GDP", "Health", "SocialSupport", "Score Level"])

# Check if the new dictionary looks right
class_happiness_df.head()

# Split it up into examples and labels 
x_data, y_data = class_happiness_df.iloc[:, :-1].values, class_happiness_df.iloc[:, -1:].values

# Check splits
#print(x_data)
#print(y_data)

# Check the data structures
#print(type(x_data))
#print(type(y_data))

# Get training and test sets, 20% will be used for the test set
x_data_train, x_data_test, y_data_train, y_data_test = train_test_split(x_data, y_data, test_size=0.20, random_state=31)

# Check the data structures
#print(type(x_data_train))
#print(type(x_data_test))
#print(type(y_data_train))
#print(type(y_data_test))

# Visualize dataFrame
class_happiness_df.sample(frac=0.1)

"""Reference: https://www.pluralsight.com/guides/importing-and-splitting-data-into-dependent-and-independent-features-for-ml

**Decision Tree**
"""

# Import the Decision Tree 
from sklearn.tree import DecisionTreeClassifier

# Initialize classifier
classifier = DecisionTreeClassifier(max_depth=5)

# Fit to data set / Training happens here! Magic!
classifier = classifier.fit(x_data_train, y_data_train)

# Visual of the actual decision tree

from sklearn import tree
import graphviz 

dot_data = tree.export_graphviz(classifier, out_file=None, impurity=False) 
graph = graphviz.Source(dot_data) 
graph

"""*Evaluating the Results*"""

# Start by testing the test set
label_predicted = classifier.predict(x_data_test)

print(label_predicted)

# Look at the accuracy of the model
acc = classifier.score(x_data_test, y_data_test) * 100

print("Decision Tree (Depth = 5):", acc)

"""As you can see, this is not a very good accuracy rate. First, we can visualize the confusion matrix and then, optimize the classification algorithm if possible."""

# Import stuff for confusion matrix
from sklearn.metrics import confusion_matrix
import seaborn as sns

# Create confusion matrix
confusion_matrix = confusion_matrix(y_data_test, label_predicted)

# Check data structure
# print(type(confusion_matrix))

matrice_df = pd.DataFrame(
    confusion_matrix, 
    index = [lab for lab in ['1', '2', '3']], 
    columns = [col for col in ['1 (pred)', '2 (pred)', '3 (pred)']])
plt.figure(figsize = (10, 7))
sns.heatmap(matrice_df, annot=True)

"""Then, because the accuracy is so low, we can try increasing the depth of our decision tree. However, this must also take into account overfitting."""

# Initialize classifier
classifier2 = DecisionTreeClassifier(max_depth=10)

# Fit to data set / Training happens here! Magic!
classifier2 = classifier2.fit(x_data_train, y_data_train)

# Visual of the actual decision tree
dot_data_v2 = tree.export_graphviz(classifier2, out_file=None, impurity=False) 
graph = graphviz.Source(dot_data_v2) 
graph

# Start by testing the test set
label_predicted_v2 = classifier2.predict(x_data_test)

print(label_predicted_v2)

# Look at the accuracy of the model
acc_v2 = classifier2.score(x_data_test, y_data_test) * 100

print("Decision Tree (Depth = 10):", acc_v2)

# Import stuff for confusion matrix
from sklearn.metrics import confusion_matrix
import seaborn as sns

# Create confusion matrix
confusion_matrix_v2 = confusion_matrix(y_data_test, label_predicted_v2)

# Check data structure
# print(type(confusion_matrix))

matrice_df_v2 = pd.DataFrame(
    confusion_matrix_v2, 
    index = [lab for lab in ['1', '2', '3']], 
    columns = [col for col in ['1 (pred)', '2 (pred)', '3 (pred)']])
plt.figure(figsize = (10, 7))
sns.heatmap(matrice_df_v2, annot=True)

"""To avoid the risk of overfitting, this method of using decision tree will end here.

--------------------------------------------

**Naive Bayes Classification**
"""

# Import modules
from sklearn.metrics import accuracy_score
from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB

# Initialize the models
model1 = GaussianNB()
model2 = BernoulliNB()
model3 = MultinomialNB()

# Fit with training data / Training Process
model1.fit(x_data_train, y_data_train)
model2.fit(x_data_train, y_data_train)
model3.fit(x_data_train, y_data_train)

# Get predictions
pred1 = model1.predict(x_data_test)
pred2 = model2.predict(x_data_test)
pred3 = model3.predict(x_data_test)

# Compute accuracies
acc1 = accuracy_score(y_data_test, pred1) * 100
acc2 = accuracy_score(y_data_test, pred2) * 100
acc3 = accuracy_score(y_data_test, pred3) * 100

# Print out accuracies
print("Gaussian Accuracy:", acc1)
print("Bernoulli Accuracy:", acc2)
print("Multinomial Accuracy:", acc3)

"""--------------------------------------------
**Support Vector Machine**
"""

# Import module
from sklearn import svm

# Initialize the model
model4 = svm.LinearSVC()
model4.fit(x_data_train, y_data_train)

# Make predictions
pred4 = model4.predict(x_data_test)

# Calculate accuracy
acc4 = accuracy_score(y_data_test, pred4) * 100
print("SVM:", acc4)

"""-------------------------------------
**Neural Network**
"""

# Import module
from sklearn.neural_network import MLPClassifier

# Initialize model
model5 = MLPClassifier(hidden_layer_sizes=(8,8,8), activation='relu', solver='adam', max_iter=500)
model5.fit(x_data_train, y_data_train)

# Make predictions
pred5 = model5.predict(x_data_test)

# Compute accuracy
acc5 = accuracy_score(y_data_test, pred5) * 100
print("Relu Neural Network:", acc5)

# Initialize model
model6 = MLPClassifier(hidden_layer_sizes=(8,8,8), activation='logistic', solver='adam', max_iter=500)
model6.fit(x_data_train, y_data_train)

# Make predictions
pred6 = model6.predict(x_data_test)

# Compute accuracy
acc6 = accuracy_score(y_data_test, pred6) * 100
print("Logistic Neural Network:", acc6)

# Initialize model
model7 = MLPClassifier(hidden_layer_sizes=(8,8,8), activation='tanh', solver='adam', max_iter=500)
model7.fit(x_data_train, y_data_train)

# Make predictions
pred7 = model7.predict(x_data_test)

# Compute accuracy
acc7 = accuracy_score(y_data_test, pred7) * 100
print("Hyperbolic Tangent Neural Network:", acc7)

print("Decision Tree (Depth = 5):", acc)
print("Decision Tree (Depth = 10):", acc_v2)
print("Gaussian Accuracy:", acc1)
print("Bernoulli Accuracy:", acc2)
print("Multinomial Accuracy:", acc3)
print("SVM:", acc4)
print("Relu Neural Network:", acc5)
print("Logistic Neural Network:", acc6)
print("Hyperbolic Tangent Neural Network:", acc7)

"""Thus, after testing all these different classification algorithms, it is clear that the Neural Network algorithm has the highest accuracy. The Bernoulli algorthm is not suited for classifying this data.

Because the tanh neural network performed the best currently, I will be trying to improve that accuracy.
"""

# List of accuracies
accuracies = []

for i in range(10):
  # Initialize model
  model8 = MLPClassifier(hidden_layer_sizes=(15,15,15,15,15,8,8,8), activation='tanh', solver='adam', max_iter=1000)
  model8.fit(x_data_train, y_data_train)

  # Make predictions
  pred8 = model8.predict(x_data_test)

  # Compute accuracy
  acc8 = accuracy_score(y_data_test, pred8) * 100
  print("Hyperbolic Tangent Neural Network:", acc8)

  accuracies.append(acc8)

print(accuracies)

"""## ***REGRESSION OBJECTIVE 1: PREDICT HAPPINESS SCORE***

My objective/goal is the regression problem: to predict a country’s happiness score. Specifically, given data about a country’s GDP per capita, social support, and healthy life expectancy. My algorithm will predict the country’s happiness score. I intend to use linear regression in 4 dimensions to predict this value.
"""

# Import statements
import numpy as np
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import time
from matplotlib import cm

regress_happiness_df = pd.DataFrame(happiness_df, columns=["Score", "GDP", "Health", "SocialSupport"])
regress_happiness_df.head()

# Slice the data into x and y.
regress_x_data, regress_y_data = regress_happiness_df.iloc[:, 1:].values, regress_happiness_df.iloc[:, :-3].values

# Check to make sure the data is sliced correctly.
#print(regress_x_data)
#print(regress_y_data)

# Split the regression data into train and test. (20% used for test)
regress_x_train, regress_x_test, regress_y_train, regress_y_test = train_test_split(regress_x_data, regress_y_data, test_size=0.20, random_state=31)

regress_happiness_df.sample(frac=0.1)

# Initialize the classifier
linear_cla = LinearRegression()

# Check the data structures
#print(type(regress_x_test))

# Create lists of x data for each tested feature
GDP_x_test = []
Health_x_test = []
SocialSupport_x_test = []

for i in range(len(regress_x_test)):
  GDP_x_test.append(regress_x_test[i][0])
  Health_x_test.append(regress_x_test[i][1])
  SocialSupport_x_test.append(regress_x_test[i][-1])

'''
print(GDP_x_test)
print(Health_x_test)
print(SocialSupport_x_test)
'''

# Fit the data / Training Process
linear_cla.fit(regress_x_train, regress_y_train)

'''
linear_cla1.fit(GDP_x_train, y_data_train)
logistic_cla1.fit(GDP_x_train, y_data_train)

linear_cla2.fit(Health_x_train, y_data_train)
logistic_cla2.fit(Health_x_train, y_data_train)

linear_cla3.fit(SocialSupport_x_train, y_data_train)
logistic_cla3.fit(SocialSupport_x_train, y_data_train)
'''

# Predictions
linear_pre = linear_cla.predict(regress_x_test)
print(linear_pre)

# Plot in 3D
ax = plt.axes(projection='3d')

# Scatter plot it
'''
plt.scatter(x_data_test, y_data_test, color='black', )
'''
ax.scatter3D(GDP_x_test, Health_x_test, SocialSupport_x_test, c=SocialSupport_x_test, cmap='Blues')

# Regression Line
def my_predict(x, y):
  return (linear_cla.coef_[0][0] / linear_cla.coef_[0][-1]) * x + (linear_cla.coef_[0][1] / linear_cla.coef_[0][-1]) * y

xs = np.linspace(0, 2.0)
ys = np.linspace(0, 2.0)
zs = my_predict(xs, ys)
ax.plot(xs, ys, zs)

ax.set_xlabel(' x = GDP')
ax.set_ylabel('y = Health')
ax.set_zlabel('z = Social Support')

# Add linear predictions
#ax.plot3D(x_data_test, linear_pre, color='c')
#print(linear_cla.coef_)

'''
# Add logistic predictions
ax.plot3D(x_data_test, logistic_pre, color='m')
'''

plt.show()

"""Reference for 3D plot: https://jakevdp.github.io/PythonDataScienceHandbook/04.12-three-dimensional-plotting.html"""

from sklearn.metrics import mean_squared_error

# Calculate the MSE 
linear_MSE = mean_squared_error(regress_y_test, linear_pre)

print("Linear MSE:", linear_MSE)

"""Now with input, we will calculate the predicted happiness score."""

regress_happiness_input = np.array([[0, 0, 0]])
GDP_ha_input = float(input("Enter the value for GDP per capita: "))
Health_ha_input = float(input("Enter the value for Health: "))
SocialSupp_ha_input = float(input("Enter the value for Social Support: "))

regress_happiness_input[0][0] = (GDP_ha_input)
regress_happiness_input[0][1] = (Health_ha_input)
regress_happiness_input[0][2] = (SocialSupp_ha_input)
regress_happiness_input.reshape(1, -1)

pre_ha_score = linear_cla.predict(regress_happiness_input)
print("The predicted happiness score for this country is %f" % pre_ha_score)

"""## ***REGRESSION OBJECTIVE 2: IMPORTANCE OF FEATURES***

My objective is another regression problem: to determine the relative importance of the features in constructing the happiness score. This may require all the features of the data and my algorithm will use the linear regression in 7 dimensions and these corresponding values for the weights (w1, w2, w3, w4, w5, w6) will be the relative importance of each feature on the happiness score.
"""

# Import
from sklearn.ensemble import ExtraTreesClassifier

# Get all columns for the 6 features
importance_feat = pd.DataFrame(happiness_df, columns=["GDP", "SocialSupport", "Health", "Freedom", "Generosity", "Corruption", "Score Level"])

importance_feat.head()

# Slice the data into x and y.
imp_feat_x_data, imp_feat_y_data = importance_feat.iloc[:, :-1].values, importance_feat.iloc[:, -1:].values

# Check to make sure the data is sliced correctly.
#print(imp_feat_x_data)
#print(imp_feat_y_data)

# Initialize Classifier
feat_imp_model = ExtraTreesClassifier()

# Fit the model
feat_imp_model.fit(imp_feat_x_data, imp_feat_y_data)

# Get coefficients to determine importance
imp_coefficients = feat_imp_model.feature_importances_
print(imp_coefficients)

# Print out coefficients
for i in range(len(imp_coefficients)):
  print("Feature: %d, Importance = %.5f" % (i, imp_coefficients[i]))

"""**END**"""